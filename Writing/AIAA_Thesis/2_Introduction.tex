\lettrine{T}{he} motivation for reusable rockets is to reduce cost access to space and to increase the number of launches per year through rapid reusability. The reduced cost makes space more accessible to all, having potential economic, social and environmental benefits. For example, Earth-imaging satellites in Brazil \footnote{Maxar Blog, ``Combating Illegal Gold Mining in the Amazon Rainforest With Maxar's High-Resolution Satellite Imagery,'' 15~February~2022, \url{https://blog.maxar.com/earth-intelligence/2022/combating-illegal-gold-mining-in-the-amazon-rainforest-with-maxars-high-resolution-satellite-imagery} (accessed 26~May~2025).}. are used to find illegal mining operations affecting the Indigenous communities. Earth imaging satellites allow for prediction of natural disasters and provide data on climate change gathering data to create crucial policies around sustainability. Furthermore, easier access to space also allow space agencies like NASA and ESA to seriously consider planetary exploration.

NASA's Perseverance Rover landed on Mars in 2021 \footnote{M.~B.~Chappell, P.~R.~P.~Chai and M.~A.~Rucker, 
  ``Mars Communications Disruption and Delay,'' NASA White Paper, 
  Document ID NASA-20230012950, 19~Jan.~2024, 
  \url{https://ntrs.nasa.gov/citations/20230012950} (accessed 26~May~2025).}
with an autonomous landing, as Mars has a communication delay which can span 23 minutes real-time control from human's on a Earth ground station is not possible. Mars provides additional challenges due to a thin atmosphere diminishing the aerodynamic braking through parachutes, and also the control authority of aerodynamic control surfaces, resulting in a landing largely dependent on thrust vectored control. Current planetary body landing tasks outside of Earth have been on Mars, Titan, the Moon and Asteroids (\cite{Blackmore2016}), but a a rocket is much heavier than the scientific payloads deployed in those missions. Also, the rocket needs to land exactly on the landing pad. Over the time land accuracy has increased, \cite{Acikmese2007} notes how tha 1997 Mars Pathfinder mission, 2004 Mars Exploration Rovers, and the 2011 Mars Curiosity rover missions have landing accuracy of 150, 35 and 10km; showing an improvement but still not accurate enough for a small launch pad.

Literature shows multiple examples of loss-less convex optimisation used for trajectory optimisation of powered descent. Successful Mars landing has been simulated using this method to minimise propellant consumption \cite{Acikmese2007}, \cite{Acikmese2013} through thrust vectored control. Minimum landing-error approach on Mars has also been researched \cite{Blackmore2010} to minimise the final distance to the landing site when the rocket is flying in a regime with a non feasible trajectory due to the constraints given. \cite{shen2022realtime} used deep neural networks to generate initial guesses for the solution of convex optimisation to reduce its computational demands, allowing for better real-time feasibility.

Convex model predictive control (MPC) \cite{Wang2018} is a proven solution for rocket landing using a finite time horizon. MPC performs well at solving constrained optimisation problems, with the convex extension convexifying constraints. \cite{Wang2018} repeatedly linearised the non-convex constraint to make them convex, using this with the MPC's finite time horizon optimisation to minimise usage of tracking error controls.

% Why reinforcement learning? History of RL
Machine learning techniques provide a useful solution to the guidance, navigation and control of rocket landing. Through deep reinforcement learning \cite{sutton1998reinforcement} an optimal policy can be mapped from states, sensor readings, to actions, actuator commands. The use of neural networks as function approximators \cite{sutton2000policy} create non-linear mappings allowing of non-linear dynamics to be controlled \cite{mnih2015human}. Online learning, updating the policy dynamically to adapt to the environment in real-time, allowing the agent to change its policy to unlock new experiences as the rocket descends. Offpolicy methods increase sample efficiency through storing previous transitions in a replay buffer.

\cite{Gaudet2018} used proximal policy optimisation as the reinforcement learning algorithm to learn the rocket landing problem on a 6 degree of freedom powered descent problem. Monte Carlo simulations were used to validate accurate landing robust to uncertainties; showing feasibility to the method application.

Focusing on an Earth landing, the rocket has different flight phases \cite{Botelho2022} where the goals and actuators used are different. With each phase flowing the previous phase they cannot be assumed immutable. Most work on using reinforcement learning for landing considers the final powered descent phase \cite{Gaudet2018}, however for true optimisation the entire trajectory must be taken. In this work, a solution to the landing problem problem is given not only considering the powered descent phase, but also the ballistic arc and boostback burn the rocket performs; encompassing all phases of the landing trajectory.

% Add more literature on the rocket landing problem with RL

This article starts by providing background to reinforcement learning approaches applicable to this problem in \autoref{sec:rl}, before describing the characteristics of the flight phases in \autoref{sec:flight_phases}. A problem formulation is formed from this background, showing the methodology to solve the problem and the simulation model derived. \autoref{sec:results} shows the results from the learnt policy for powered descent and the trajectory generated for the previous flight phases. Finally, \autoref{sec:conclusion} evaluates the results to form a conclusion and presents future work opportunities.
