@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT press}
}

@inproceedings{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  booktitle={Nature},
  year={2015},
  pages={529-533}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996},
  publisher={Journal of Artificial Intelligence Research}
}

@book{puterman1994markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  volume={12},
  pages={1057--1063},
  year={2000}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  volume={12},
  pages={1008--1014},
  year={2000}
}

@techreport{thrun1992efficient,
  title={Efficient exploration in reinforcement learning},
  author={Thrun, Sebastian},
  year={1992},
  institution={Carnegie Mellon University},
  type={Technical Report CMU-CS-92-102}
}

@article{auer2002using,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@misc{mnih2013playing,
  title={Playing Atari with Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year={2013},
  eprint={1312.5602},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  institution={DeepMind Technologies}
}

@inproceedings{vanhasselt2016deep,
  title={Deep Reinforcement Learning with Double Q-Learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016},
  organization={AAAI Press},
  pages={2094--2100},
  publisher={AAAI Press}
}


@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning (ICML)},
  year={2016},
  pages={1995--2003},
  organization={PMLR}
}

@article{schaul2015prioritized,
  title={Prioritized Experience Replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven Exploration by Self-supervised Prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2010},
  publisher={IEEE}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  pages={1633--1685},
  year={2009},
  publisher={MIT Press}
}

@inproceedings{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS)},
  pages={1--10},
  year={2016},
  organization={Google Deep Mind}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight Experience Replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  url={https://arxiv.org/abs/1707.01495}
}

@article{lillicrap2015continuous,
  title={Continuous Control with Deep Reinforcement Learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and others},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015},
  url={https://arxiv.org/abs/1509.02971}
}

@inproceedings{plappert2018parameter,
  title={Parameter Space Noise for Exploration},
  author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and others},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://arxiv.org/abs/1706.01905}
}

@article{sehnke2010parameter,
  title={Parameter-exploring policy gradients},
  author={Sehnke, Frank and Osendorfer, Christian and Rückstieß, Thomas and others},
  journal={Neural Networks},
  volume={23},
  number={4},
  pages={551--559},
  year={2010},
  publisher={Elsevier},
  doi={10.1016/j.neunet.2009.12.004}
}

@inproceedings{rucksties2008state,
  title={State-Dependent Exploration for Policy Gradient Methods},
  author={R{\"u}ckstie{\ss}, Thomas and Felder, Martin and Schmidhuber, J{\"u}rgen},
  booktitle={European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={234--249},
  year={2008},
  publisher={Springer},
  doi={10.1007/978-3-540-87481-2_31}
}

@inproceedings{burda2018exploration,
  title={Exploration by Random Network Distillation},
  author={Burda, Yuri and Edwards, Harrison and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://arxiv.org/abs/1810.12894}
}


@article{fortunato2017noisy,
  title={Noisy Networks for Exploration},
  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, R{\'e}mi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles},
  journal={arXiv preprint arXiv:1706.10295},
  year={2017}
}


@article{yin2022distributed,
  title={Distributed Deep Reinforcement Learning: A Survey and a Multi-Player Multi-Agent Learning Toolbox},
  author={Yin, Qiyue and others},
  journal={arXiv preprint arXiv:2212.00253},
  year={2022}
}

@article{schulman2017proximal,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{espeholt2018impala,
  title={IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@article{mnih2016asynchronous,
  title={Asynchronous Methods for Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1602.01783},
  year={2016}
}


@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group},
  doi={10.1038/s41586-019-1724-z}
}

@article{horgan2018distributed,
  title={Distributed Prioritized Experience Replay},
  author={Horgan, Daniel and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}

@inproceedings{silver2014deterministic,
  title={Deterministic Policy Gradient Algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML)},
  volume={32},
  number={1},
  pages={387--395},
  year={2014},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{barthmaron2018d4pg,
  title={Distributed Distributional Deterministic Policy Gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and TB, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2018},
  organization={DeepMind}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@inproceedings{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{wahid2020learning,
  title={Learning Object-conditioned Exploration using Distributed Soft Actor Critic},
  author={Wahid, Ayzaan and Stone, Austin and Chen, Kevin and Ichter, Brian and Toshev, Alexander},
  journal={arXiv preprint arXiv:2007.14545},
  year={2020}
}

@article{akimov2019distributed,
  title={Distributed Soft Actor-Critic with Multivariate Reward Representation and Knowledge Distillation},
  author={Akimov, Dmitry},
  journal={arXiv preprint arXiv:1911.13056},
  year={2019}
}

@article{abdolmaleki2018maximum,
  title={Maximum a Posteriori Policy Optimisation},
  author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
  journal={arXiv preprint arXiv:1806.06920},
  year={2018}
}

@misc{prior2024jaxrl,
  author       = {Henry Prior},
  title        = {jax-rl: MPO.py},
  year         = {2024},
  howpublished = {\url{https://github.com/henry-prior/jax-rl/blob/master/jax_rl/MPO.py}},
  note         = {Accessed: 22-Nov-2024}
}

@article{song2019vmpo,
  title     = {V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control},
  author    = {H. Francis Song and Abbas Abdolmaleki and Jost Tobias Springenberg and Aidan Clark and Hubert Soyer and Jack W. Rae and Seb Noury and Arun Ahuja and Siqi Liu and Dhruva Tirumala and Nicolas Heess and Dan Belov and Martin Riedmiller and Matthew M. Botvinick},
  journal   = {arXiv preprint arXiv:1909.12238},
  year      = {2019},
  url       = {https://arxiv.org/abs/1909.12238}
}

@article{hoffman2020acme,
  title     = {Acme: A Research Framework for Distributed Reinforcement Learning},
  author    = {Hoffman, Matthew W. and Shahriari, Bobak and Aslanides, John and Barth-Maron, Gabriel and Momchev, Nikola and Sinopalnikov, Danila and Stańczyk, Piotr and Ramos, Sabela and Raichuk, Anton and Vincent, Damien and Hussenot, Léonard and Dadashi, Robert and Dulac-Arnold, Gabriel and Orsini, Manu and Jacq, Alexis and Ferret, Johan and Vieillard, Nino and Seyed Ghasemipour, Seyed Kamyar and Girgin, Sertan and Pietquin, Olivier and Behbahani, Feryal and Norman, Tamara and Abdolmaleki, Abbas and Cassirer, Albin and Yang, Fan and Baumli, Kate and Henderson, Sarah and Friesen, Abe and Haroun, Ruba and Novikov, Alex and Gómez Colmenarejo, Sergio and Cabi, Serkan and Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Cowie, Andrew and Wang, Ziyu and Piot, Bilal and de Freitas, Nando},
  journal   = {arXiv preprint arXiv:2006.00979},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.00979}
}

@misc{ding,
    title={DI-engine: A Universal AI System/Engine for Decision Intelligence},
    author={Niu, Yazhe and Xu, Jingxin and Pu, Yuan and Nie, Yunpeng and Zhang, Jinouwen and Hu, Shuai and Zhao, Liangxuan and Zhang,  Ming and Liu, Yu},
    publisher={GitHub},
    howpublished={\url{https://github.com/opendilab/DI-engine}},
    year={2021},
}

@misc{zhou2024distributed,
  author       = {Henry Zhou},
  title        = {Distributed Distributional DrQ},
  year         = {2024},
  url          = {https://github.com/zhou-henry/distributed-distributional-drq},
  note         = {Accessed: 22-Nov-2024}
}

@article{pardo2020tonic,
  title={Tonic: A Deep Reinforcement Learning Library for Fast Prototyping and Benchmarking},
  author={Pardo, Fabio},
  journal={arXiv preprint arXiv:2011.07537},
  year={2020},
  url={https://github.com/fabiopardo/tonic}
}

@InProceedings{nicolas2020CombinationDDPGTD3,
author="Bach, Nicolas
and Melnik, Andrew
and Schilling, Malte
and Korthals, Timo
and Ritter, Helge",
editor="Nicosia, Giuseppe
and Ojha, Varun
and La Malfa, Emanuele
and Jansen, Giorgio
and Sciacca, Vincenzo
and Pardalos, Panos
and Giuffrida, Giovanni
and Umeton, Renato",
title="Learn to Move Through a Combination of Policy Gradient Algorithms DDPG, D4PG, and TD3",
booktitle="Machine Learning, Optimization, and Data Science",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="631--644",
}

@article{pinto2017robust,
  title={Robust Adversarial Reinforcement Learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  journal={arXiv preprint arXiv:1703.02702},
  year={2017}
}

@article{pan2019rararl,
  title={Risk-Averse Robust Adversarial Reinforcement Learning},
  author={Pan, Xinlei and Seita, Daniel and Gao, Yang and Canny, John},
  journal={arXiv preprint arXiv:1904.00511},
  year={2019}
}

@article{reddi2023quantal,
  title={Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula},
  author={Reddi, Aryaman and T{\"o}lle, Maximilian and Peters, Jan and Chalvatzaki, Georgia and D’Eramo, Carlo},
  journal={arXiv preprint arXiv:2311.01642},
  year={2023}
}

@inbook{Heifetz_2012, place={Cambridge}, title={Nash equilibrium}, booktitle={Game Theory: Interactive Strategies in Economics and Management}, publisher={Cambridge University Press}, author={Heifetz, Aviad}, editor={Yalon-Fortus, JudithTranslator}, year={2012}, pages={65–84}}

@inproceedings{mandlekar2017arpl,
  title={Adversarially Robust Policy Learning through Active Construction of Physically-Plausible Perturbations},
  author={Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Fei-Fei, Li and Savarese, Silvio},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year={2017},
  organization={JMLR: W\&CP}
}

@article{christiano2016transfer,
  title={Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Models},
  author={Christiano, Paul and Shah, Zain and Mordatch, Igor and Schneider, Jonas and Blackwell, Trevor and Tobin, Joshua and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1610.03518},
  year={2016}
}

@inproceedings{rusu2017sim2real,
  title={Sim-to-real robot learning from pixels with progressive nets},
  author={Rusu, Andrei A and Vecerik, Mel and Rothörl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
  booktitle={Conference on Robot Learning (CoRL)},
  pages={262--270},
  year={2017},
  organization={PMLR}
}

@inproceedings{rajeswaran2017epopt,
  title={EPOpt: Learning robust neural network policies using model ensembles},
  author={Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@book{deb2001multi,
  title={Multi-Objective Optimization Using Evolutionary Algorithms},
  author={Deb, Kalyanmoy},
  year={2001},
  publisher={Wiley}
}

@book{eiben2003introduction,
  title={Introduction to Evolutionary Computing},
  author={Eiben, A.E. and Smith, J.E.},
  year={2003},
  publisher={Springer}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for Inverse Reinforcement Learning},
  author={Ng, Andrew Y and Russell, Stuart J},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  pages={663--670},
  year={2000},
}

@article{adams2022survey,
  title={A survey of inverse reinforcement learning},
  author={Adams, Stephen and Cody, Tyler and Beling, Peter A},
  journal={Artificial Intelligence Review},
  volume={55},
  pages={4307--4346},
  year={2022},
  publisher={Springer},
  doi={10.1007/s10462-021-10108-x}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning},
  author={Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
  booktitle={Proceedings of the 23rd AAAI Conference on Artificial Intelligence},
  pages={1433--1438},
  year={2008},
  organization={AAAI Press}
}

@misc{qzed_maxent_irl,
  author       = {Qzed},
  title        = {Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL)},
  year         = {2021},
  howpublished = {\url{https://github.com/qzed/irl-maxent}},
  note         = {Accessed: 2024-11-25}
}

@article{zheng2022imitation,
  title={Imitation Learning: Progress, Taxonomies and Challenges},
  author={Zheng, Boyuan and Verma, Sunny and Zhou, Jianlong and Tsang, Ivor and Chen, Fang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  month={October},
  abstract={This survey provides a systematic review of imitation learning, its taxonomies, key milestones, challenges, and future directions, addressing issues like learning policies from suboptimal demonstrations, voice instructions, and optimization schemes.},
  url={https://arxiv.org/abs/2106.12177}
}

@inproceedings{ho2016generative,
  title={Generative Adversarial Imitation Learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={4565--4573},
  year={2016},
  publisher={Curran Associates, Inc.},
  url={http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf}
}

@article{hospedales2020meta,
  title={Meta-Learning in Neural Networks: A Survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}

@article{vettoruzzo2023advances,
  title={Advances and Challenges in Meta-Learning: A Technical Review},
  author={Vettoruzzo, Anna and Bouguelia, Mohamed-Rafik and Vanschoren, Joaquin and R{\"o}gnvaldsson, Thorsteinn and Santosh, KC},
  journal={arXiv preprint arXiv:2307.04722},
  year={2023}
}

@inproceedings{schulman2015trpo,
  title={Trust Region Policy Optimization},
  author={John Schulman and Sergey Levine and Philipp Moritz and Michael Jordan and Pieter Abbeel},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML)},
  year={2015},
  pages={1889--1897},
  publisher={JMLR}
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}

@article{mnih2016asynchronous,
  author       = {Volodymyr Mnih and
                  Adri{\`{a}} Puigdom{\`{e}}nech Badia and
                  Mehdi Mirza and
                  Alex Graves and
                  Timothy P. Lillicrap and
                  Tim Harley and
                  David Silver and
                  Koray Kavukcuoglu},
  title        = {Asynchronous Methods for Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1602.01783},
  year         = {2016},
  url          = {http://arxiv.org/abs/1602.01783},
  eprinttype    = {arXiv},
  eprint       = {1602.01783},
  timestamp    = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{suttle2019multi,
  title={A Multi-Agent Off-Policy Actor-Critic Algorithm for Distributed Reinforcement Learning},
  author={Suttle, Wesley and others},
  journal={arXiv preprint arXiv:1903.06372},
  year={2019},
  url={https://arxiv.org/abs/1903.06372}
}

@article{zhang2019distributed,
  title={Distributed Off-Policy Actor-Critic Reinforcement Learning with Policy Consensus},
  author={Zhang, Yan and Zavlanos, Michael M.},
  journal={arXiv preprint arXiv:1903.09255},
  year={2019},
  url={https://arxiv.org/abs/1903.09255}
}

@article{ren2024multi,
  title={Multi-Agent Gradient-Based Off-Policy Actor-Critic Algorithm for Distributed Reinforcement Learning},
  author={Ren, Jineng},
  journal={International Journal of Automation and Computing},
  year={2024},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s44196-024-00560-2}
}

@misc{doan2023bayesianNN,
  title={Introduction to Bayesian Neural Networks},
  author={Anh Khoa Doan},
  year={2023},
  note={Lecture slides, TU Delft},
  url={https://aerodynamics.lr.tudelft.nl/~rdwight/cfdiv/Videos/08/slides/02_UQ_BayesianNN.pdf}
}

@article{tunyasuvunakool2020,
         title = {dm\_control: Software and tasks for continuous control},
         journal = {Software Impacts},
         volume = {6},
         pages = {100022},
         year = {2020},
         issn = {2665-9638},
         doi = {https://doi.org/10.1016/j.simpa.2020.100022},
         author = {Saran Tunyasuvunakool and Alistair Muldal and Yotam Doron and
                   Siqi Liu and Steven Bohez and Josh Merel and Tom Erez and
                   Timothy Lillicrap and Nicolas Heess and Yuval Tassa},
}


@article{Blackmore2016,
  author = {Lars Blackmore},
  title = {Autonomous Precision Landing of Space Rockets},
  journal = {The Bridge},
  year = {2016},
  volume = {46},
  number = {4},
  pages = {16--22}
}

@article{Acikmese2007,
  author = {Behçet Açıkmeşe and Scott R. Ploen},
  title = {Convex Programming Approach to Powered Descent Guidance for Mars Landing},
  journal = {Journal of Guidance, Control, and Dynamics},
  volume = {30},
  number = {5},
  pages = {1353--1366},
  year = {2007},
  doi = {10.2514/1.27553},
}

@inproceedings{Wang2018,
  author = {Cong Wang and Zhengyu Song},
  title = {Convex Model Predictive Control for Rocket Vertical Landing},
  booktitle = {Proceedings of the 37th Chinese Control Conference},
  year = {2018},
  address = {Wuhan, China},
  pages = {9837--9843},
}

@article{Blackmore2010,
  author = {Lars Blackmore and Behçet Açıkmeşe and Daniel P. Scharf},
  title = {Minimum-Landing-Error Powered-Descent Guidance for Mars Landing Using Convex Optimization},
  journal = {Journal of Guidance, Control, and Dynamics},
  volume = {33},
  number = {4},
  pages = {1161--1171},
  year = {2010},
  doi = {10.2514/1.47202},
}

@article{Yu2020,
  author = {Junhao Yu and Jiarun Wei},
  title = {Rocket Landing Control with Grid Fins and Path-following using MPC},
  journal = {University of California, Berkeley, ME C231A Final Project Report},
  year = {2020},
  note = {Available online: https://arxiv.org/abs/2012.00214},
}

@article{Wang2020,
  author = {Jinbo Wang and Huixu Li and Hongbo Chen},
  title = {An Iterative Convex Programming Method for Rocket Landing Trajectory Optimization},
  journal = {The Journal of the Astronautical Sciences},
  volume = {67},
  pages = {1553--1574},
  year = {2020},
  doi = {10.1007/s40295-020-00235-y},
}

@mastersthesis{Ferrante2017,
  author = {Reuben Ferrante},
  title = {A Robust Control Approach for Rocket Landing},
  school = {University of Edinburgh, School of Informatics},
  year = {2017},
  note = {Available online: \url{https://doi.org/10.7488/ds/2306}},
}

@article{Acikmese2013,
  author = {Behçet Açıkmeşe and John M. Carson III and Lars Blackmore},
  title = {Lossless Convexification of Nonconvex Control Bound and Pointing Constraints of the Soft Landing Optimal Control Problem},
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {21},
  number = {6},
  pages = {2104--2113},
  year = {2013},
  doi = {10.1109/TCST.2012.2237346},
}

@misc{NASA2023,
  author = {{National Aeronautics and Space Administration (NASA)}},
  title = {Mars Communications Disruption and Delay},
  howpublished = {Moon to Mars Architecture Concept Review},
  year = {2023},
  note = {Available online: {https://www.nasa.gov/feature/moon-to-mars-architecture-concept-review-2023}},
}

@article{Jiang2024,
  author    = {Yuxuan Jiang and Yujie Yang and Zhiqian Lan and Guojian Zhan and Shengbo Eben Li and Qi Sun and Jian Ma and Tianwen Yu and Changwu Zhang},
  title     = {Rocket Landing Control with Random Annealing Jump Start Reinforcement Learning},
  year      = {2024},
  url       = {https://arxiv.org/abs/2407.15083}
}

@article{Yang2022,
  author    = {Yujie Yang and Zhiqian Lan and Shengbo Eben Li et al.},
  title     = {Real-Time Guidance for Powered Landing of Reusable Rockets via Deep Reinforcement Learning},
  journal   = {Neural Computing and Applications},
  volume    = {34},
  pages     = {12345--12356},
  year      = {2022},
  doi       = {10.1007/s00521-022-08024-4},
  url       = {https://link.springer.com/article/10.1007/s00521-022-08024-4}
}

@article{Malcolm2023,
  author    = {Alexander Malcolm and Luis Casco-Rodriguez},
  title     = {A Review of Spiking Neural Networks: Interpretation, Optimization, and Best Practices},
  journal   = {arXiv preprint arXiv:2303.10780},
  year      = {2023},
  url       = {https://arxiv.org/abs/2303.10780}
}

@misc{cmu2021recitation,
  title={Recitation 5: A Comparative Overview of DP vs. MC vs. TD},
  author={Carnegie Mellon University},
  year={2021},
}

@article{Gaudet2018,
    author    = {Brian Gaudet and Richard Linares and Roberto Furfaro},
    title     = {Deep Reinforcement Learning for Six Degree-of-Freedom Planetary Powered Descent and Landing},
    journal   = {arXiv preprint arXiv:1810.08719},
    year      = {2018},
    url       = {https://arxiv.org/abs/1810.08719},
    note      = {Accessed from arXiv}
}

@techreport{boyd2003socp,
  author = {Stephen Boyd},
  title = {Second-Order Cone Programming},
  institution = {Stanford University},
  year = {2003},
  url = {https://web.stanford.edu/~boyd/papers/pdf/socp.pdf},
  note = {Accessed: 2024-12-02}
}

@inproceedings{Szmuk_2018,
   title={Successive Convexification for 6-DoF Mars Rocket Powered Landing with Free-Final-Time},
   url={http://dx.doi.org/10.2514/6.2018-0617},
   DOI={10.2514/6.2018-0617},
   booktitle={2018 AIAA Guidance, Navigation, and Control Conference},
   publisher={American Institute of Aeronautics and Astronautics},
   author={Szmuk, Michael and Acikmese, Behcet},
   year={2018},
   month=jan }

@article{ping2017optimalguidance,
author = {Lu, Ping},
year = {2017},
month = {12},
pages = {1-14},
title = {Propellant-Optimal Powered Descent Guidance},
volume = {41},
journal = {Journal of Guidance, Control, and Dynamics},
doi = {10.2514/1.G003243}
}

@article{mao2017successive,
  author = {Yuanqi Mao and Daniel Dueri and Michael Szmuk and Beh\c{e}t Ac\i{k}me\c{s}e},
  title = {Successive Convexification of Non-Convex Optimal Control Problems with State Constraints},
  journal = {arXiv:1701.00558v2},
  year = {2017},
  url = {https://arxiv.org/abs/1701.00558v2},
  note = {Accessed: 2024-12-02},
  institution = {University of Washington, Seattle, WA, USA}
}

@misc{mars_rocket_image,
  title        = {Reusable Rocket Launch on Mars-like Surface},
  author       = {Generated with OpenAI DALL-E},
  year         = {2024},
  note         = {Custom-generated image featuring a reusable rocket launch with landing legs, small concrete pad, and Mars-like environment. Created for academic purposes.}
}

@article{Botelho2022,
  author    = {Afonso Botelho and Marc Martinez and Cristina Recupero and Andrea Fabrizi and Gabriele De Zaiacomo},
  title     = {Design of the landing guidance for the retro-propulsive vertical landing of a reusable rocket stage},
  journal   = {CEAS Space Journal},
  volume    = {14},
  pages     = {551--564},
  year      = {2022},
  publisher = {Springer},
  doi       = {10.1007/s12567-022-00423-6},
  url       = {https://doi.org/10.1007/s12567-022-00423-6},
  abstract  = {This paper presents the GNC solution currently in development by DEIMOS Space for RETALT, an EU Horizon 2020 funded project for studying launch system reusability technologies for different classes of vertical take-off vertical-landing vehicles. The guidance strategy is based on direct optimal control methods via on-board optimization, necessary to achieve the landing accuracy required for a reusable rocket stage.},
}

@article{shen2022realtime,
  title={Real-time computational powered landing guidance using convex optimization and neural networks},
  author={Shen, Zhipeng and Zhou, Shiyu and Yu, Jianglong},
  journal={arXiv preprint arXiv:2210.07480},
  year={2022}
}

@article{dezaiacomo2022retalt,
  title={Mission engineering for the RETALT VTVL launcher},
  author={De Zaiacomo, Gabriele and Blanco Arnao, Gonzalo and Bunt, Riccardo and Bonetti, Davide},
  journal={CEAS Space Journal},
  volume={14},
  pages={533--549},
  year={2022},
  publisher={Springer},
  doi={10.1007/s12567-021-00415-y}
}

@article{Sohaib2018MPC,
author = {Tahir, Sohaib and Wang, Jie and Baloch, Mazhar and Kaloi, Ghulam},
year = {2018},
month = {02},
pages = {18},
title = {Digital Control Techniques Based on Voltage Source Inverters in Renewable Energy Applications: A Review},
volume = {7},
journal = {Electronics},
doi = {10.3390/electronics7020018}
}

@book{rawlings2017mpc,
  title={Model Predictive Control: Theory, Computation, and Design},
  author={Rawlings, James B. and Mayne, David Q. and Diehl, Moritz},
  year={2017},
  publisher={Nob Hill Publishing},
  edition={2nd},
  isbn={978-0975937743}
}

@techreport{NASA2019,
  title={Lunar Pallet Lander Design: Descent and Landing Performance Analysis},
  author={NASA},
  year={2019},
  institution={NASA Technical Reports Server (NTRS)},
  url={https://ntrs.nasa.gov/api/citations/20190002101/downloads/20190002101.pdf},
  note={Accessed on [insert date]}
}

@article{certified_safe,
  author       = {Tom Jones},
  title        = {Certified Safe},
  journal      = {Smithsonian Magazine},
  year         = {2009},
  month        = {August},
  url          = {https://www.smithsonianmag.com/air-space-magazine/certified-safe-281371/},
  note         = {Accessed: 2024-12-04}
}


@misc{rocketpy,
  author       = {RocketPy-Team},
  title        = {RocketPy: Advanced Rocket Flight Simulation in Python},
  year         = {2024},
  url          = {https://github.com/RocketPy-Team/RocketPy},
  note         = {Accessed: 2024-12-07},
}

@book{hibbeler2015engineering,
  title={Engineering Mechanics: Statics},
  author={Hibbeler, R.C.},
  edition={14th},
  year={2015},
  publisher={Pearson},
  address={Upper Saddle River, NJ},
  isbn={978-0-13-391892-2}
}


@incollection{Reyhanoglu2011,
  author    = {Mahmut Reyhanoglu},
  title     = {Modeling and Control of Space Vehicles with Fuel Slosh Dynamics},
  booktitle = {Advances in Spacecraft Technologies},
  editor    = {Jason Hall},
  publisher = {InTech},
  year      = {2011},
  pages     = {1--26},  % Adjust this if specific pages are known
  isbn      = {978-953-307-551-8},
  url       = {http://www.intechopen.com/books/advances-in-spacecraft-technologies/modelling-and-control-of-space-vehicles-with-fuel-slosh-dynamics},
  note      = {Published online February 14, 2011}
}

@book{Anderson2020Introduction,
  author    = {John D Anderson Junior},
  title     = {Introduction to Flight},
  edition   = {8th},
  publisher = {McGraw-Hill Education},
  year      = {2020},
  isbn      = {978-1260012383}
}

@misc{MIT_OCW_ClassicalMechanics_2016,
  title        = {Classical Mechanics, Chapter 6},
  author       = {Massachusetts Institute of Technology},
  year         = {2016},
  note         = {MIT OpenCourseWare},
  url          = {https://ocw.mit.edu/courses/8-01sc-classical-mechanics-fall-2016/mit8_01scs22_chapter6.pdf},
}

@misc{karabeyoglu_thermochemistry_2019,
  author = {Arif Karabeyoglu},
  title = {Lecture 4: Thermochemistry and Propellants Part 1},
  year = {2019},
  howpublished = {\url{https://web.stanford.edu/~cantwell/AA284A_Course_Material/Karabeyoglu\%20AA\%20284A\%20Lectures/AA284a_Lecture4.pdf}},
  note = {Accessed: 2024-12-11}
} 

@manual{marsgram2024,
  title        = {Mars-GRAM 2024 User Guide},
  author       = {NASA},
  year         = {2024},
  url          = {https://ntrs.nasa.gov/api/citations/20240012934/downloads/Mars-GRAM%202024%20User%20Guide.pdf},
  note         = {Accessed: 2024-12-11}
}

@book{Mulder2020,
  title        = {Aircraft Responses to Atmospheric Turbulence},
  author       = {Mulder, J.A. and van der Vaart, J.C. and van Staveren, W.H.J.J. and Chu, Q.P. and Mulder, M.},
  year         = {2020},
  publisher    = {Delft University of Technology},
  address      = {Delft, The Netherlands},
  note         = {Lecture Notes AE4304}
}

@book{stewart2015calculus,
  title     = {Calculus: Early Transcendentals},
  author    = {Stewart, James},
  year      = {2015},
  edition   = {8th},
  publisher = {Cengage Learning},
  address   = {Boston, MA},
  isbn      = {978-1-285-74155-0}
}

@incollection{ZHANG201041,
title = {CHAPTER 2 - Industrial control engineering},
editor = {Peng Zhang},
booktitle = {Advanced Industrial Control Technology},
publisher = {William Andrew Publishing},
address = {Oxford},
pages = {41-70},
year = {2010},
isbn = {978-1-4377-7807-6},
doi = {https://doi.org/10.1016/B978-1-4377-7807-6.10002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781437778076100026},
author = {Peng Zhang},
abstract = {Publisher Summary
This chapter focuses on the three areas: process control, motion control, and production automation and explains their system models, control strategies, control devices, and control implementations. Industrial control engineering is the branch of industrial engineering that deals with the design, development, and implementation of integrated systems of humans, machines, devices, and information resources which together provide applications that behave in the desired manner. Industrial control engineering encompasses both control theory and control technology. Thus, it involves modeling the spatial and temporal natures of industrial objects; analyzing the dynamic behavior of industrial objects; using control theory to create control strategies; designing control devices, hardware and software; and implementing industrial control systems. Industrial control engineering is a very complex subject, with its main areas being work methods analysis and improvement; work measurement and the establishment of standards; machine tool analysis and design; workplace design; plant layout and facility design; materials handling; cost reduction; production planning and scheduling; inventory control, maintenance, and replacement; statistical quality control; scheduling; assembly-line balancing, systems, and procedures; and overall productivity improvement.}
}


@article{LI2020105999,
title = {Conjugate gradient method with pseudospectral collocation scheme for optimal rocket landing guidance},
journal = {Aerospace Science and Technology},
volume = {104},
pages = {105999},
year = {2020},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2020.105999},
url = {https://www.sciencedirect.com/science/article/pii/S1270963820306817},
author = {Yang Li and Wanchun Chen and Hao Zhou and Liang Yang},
keywords = {Rocket landing guidance, Nonlinear optimal control, Pseudospectral collocation, Conjugate gradient method},
abstract = {This paper aims at proposing a pseudospectral collocation conjugate gradient method (PCCG) for solving the nonlinear optimal control problems with terminal constraints and to apply it to rocket landing guidance. Firstly, the projection technique and conjugate gradient method are combined to obtain the control that can satisfy the terminal constraints. In addition, a new step-size adjustment strategy is applied to make the algorithm converge faster. Secondly, to improve the computational efficiency, the nonlinear adjoint and auxiliary differential equations are transformed into a set of linear algebraic equations by using the Gauss pseudospectral collocation scheme. Therefore, the values of adjoint and auxiliary functions at LG nodes can be obtained by solving the system of linear equations, and the special integrals needed in the control update process can be solved more effectively by using the Gauss quadrature method. Finally, PCCG algorithm is applied to the rocket landing guidance problem. A comparison with conjugate gradient method and GPOPS-II is also provided. Simulation results show that the algorithm has high convergence speed and computational efficiency, and the solution is optimal.}
}

@article{KOCH201965,
title = {Optimal staging of serially staged rockets with velocity losses and fairing separation},
journal = {Aerospace Science and Technology},
volume = {88},
pages = {65-72},
year = {2019},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S1270963818325641},
author = {Aaron D. Koch},
keywords = {Multi-stage rocket, Tsiolkovsky rocket equation, Optimal staging, Velocity losses},
abstract = {Currently, algorithms exist for the optimal staging of serially staged rockets under loss-free conditions. These algorithms are based on the Tsiolkovsky rocket equation. Here, one variant is extended to include velocity losses and/or fairing separation. Instead of simply adding the velocity losses to the required loss-free Δv and freely distributing the total amount among all stages, a two-step process is implemented. First, the loss-free solution is obtained to determine the optimal velocity gain for each stage. Then, the rocket's stages are iteratively scaled up, starting with the uppermost stage and continuing downwards. The size of each stage is enlarged so that it generates Δv equal to the optimal velocity gain plus the losses occurring during its flight. Also, each stage accounts for the increased mass of the stages on top of it. Adding the velocity losses after the optimization step ensures that they are allocated to the correct stages. Ariane 40 is used as an example for a three-stage rocket. In this case, the proposed method produced realistic payload ratios, in contrast to the old idea of adding the velocity losses directly to the required loss-free Δv. As the method requires inputs that stem from a trajectory analysis, it works best when iteratively coupled to a trajectory optimizer. In doing so, Ariane 40's total payload ratio was increased, while taking both velocity losses and fairing separation into account.}
}

@article{JO2021106431,
title = {Optimal staging of reusable launch vehicles considering velocity losses},
journal = {Aerospace Science and Technology},
volume = {109},
pages = {106431},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2020.106431},
url = {https://www.sciencedirect.com/science/article/pii/S1270963820311135},
author = {Byeong-Un Jo and Jaemyung Ahn},
keywords = {Optimal sizing, Launch vehicle design, Reusable launch vehicle, Trajectory optimization, Soft landing, Return to launch site},
abstract = {This paper proposes an optimal staging method for reusable launch vehicles considering velocity losses. The proposed method expands a traditional stage optimization approach for expendable launch vehicles by introducing the descent phase of the separated stage and combining it with the trajectory optimization procedure. For a reusable stage, the structural ratios responsible for its ascent and landing phases are defined to formulate the optimal sizing problem and optimized along with variables for traditional staging problems. An optimal staging case study on a reusable launch vehicle demonstrates the effectiveness of the proposed method.}
}

@MastersThesis{Launcher_Trajectory,
    author     =     {D. M Gaspar},
    title     =     {{A Tool for Preliminary Design of Rockets}},
    school     =     {Faculty of Aerospace Engineering - Technico Lisboa},
    address     =     {Lisbon, Portugal},
    year     =     {2014},
    }


@book{sutton_rocket_2016,
  title        = {Rocket Propulsion Elements},
  author       = {Sutton, George P. and Biblarz, Oscar},
  year         = {2016},
  edition      = {9th},
  publisher    = {John Wiley \& Sons},
  address      = {Hoboken, NJ},
  isbn         = {978-1-119-17120-4}
}

@inproceedings{krogh1992weightdecay,
  author    = {Anders Krogh and Jesper A. Hertz},
  title     = {A Simple Weight Decay Can Improve Generalization},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {4},
  year      = {1992},
  pages     = {950--957},
  publisher = {Morgan Kaufmann},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1991/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf}
}

@inproceedings{andrychowicz2020matters,
  author    = {Marcin Andrychowicz and Anton Raichuk and Piotr Stanczyk and Manu Orsini and Zafrir Harchaoui and Leszek Espeholt and Raphael Marinier and Olivier Bachem and Nicolas Heess},
  title     = {What Matters in On-Policy Reinforcement Learning? A Large-Scale Empirical Study},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021},
  url       = {https://openreview.net/forum?id=r1etN1rtPB}
}

@manual{SpaceX2020_StarshipGuide,
  title         = {Starship Users Guide},
  subtitle      = {Revision 1.0},
  author        = {{Space Exploration Technologies Corp.}},
  organization  = {SpaceX},
  address       = {Hawthorne, California},
  month         = mar,
  year          = {2020},
  url           = {https://www.spacex.com/media/starship_users_guide_v1.pdf},
  note          = {Accessed 4 May 2025}
}

@article{ReusbaleStaging,
title = {Optimal staging of reusable launch vehicles considering velocity losses},
journal = {Aerospace Science and Technology},
volume = {109},
pages = {106431},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2020.106431},
url = {https://www.sciencedirect.com/science/article/pii/S1270963820311135},
author = {Byeong-Un Jo and Jaemyung Ahn},
keywords = {Optimal sizing, Launch vehicle design, Reusable launch vehicle, Trajectory optimization, Soft landing, Return to launch site},
abstract = {This paper proposes an optimal staging method for reusable launch vehicles considering velocity losses. The proposed method expands a traditional stage optimization approach for expendable launch vehicles by introducing the descent phase of the separated stage and combining it with the trajectory optimization procedure. For a reusable stage, the structural ratios responsible for its ascent and landing phases are defined to formulate the optimal sizing problem and optimized along with variables for traditional staging problems. An optimal staging case study on a reusable launch vehicle demonstrates the effectiveness of the proposed method.}
}

@techreport{TIR-33,
  title        = {{TIR-33} Rocket Stability and Control Manual},
  author       = {{Society of Amateur Rocketry and Space Modelling}},
  institution  = {Space Modeling Association},
  type         = {Technical Information Report},
  number       = {TIR-33},
  year         = {1998},
  url          = {https://www.spacemodeling.org/jimz/manuals/tir-33.pdf},
  note         = {Accessed: 2025-05-06}
}

@inproceedings{washington1993grid,
  author       = {William D. Washington and Mark S. Miller},
  title        = {Grid Fins – A New Concept for Missile Stability and Control},
  booktitle    = {Proceedings of the 31\textsuperscript{st} Aerospace Sciences Meeting \& Exhibit},
  year         = {1993},
  number       = {AIAA-93-0035},
  address      = {Reno, Nevada},
  organisation = {American Institute of Aeronautics and Astronautics},
  doi          = {10.2514/6.1993-35}
}
