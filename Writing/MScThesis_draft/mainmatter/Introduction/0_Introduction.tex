A revolutionary shift in space exploration occurred when SpaceX successfully landed the Falcon 9 rocket in 2015, making reusable rockets become an industry standard. This resulted in reduced launch costs and allowed for more economically viable space initiatives due to lower barriers of entry. Not only does cost efficiency boost private industry, but it also allows public bodies like NASA to consider planetary exploration seriously.

These landing technologies do not necessarily need to be confined to use only on Earth. As NASA edges closer to exploring Mars, control systems are required to land autonomously on extraterrestrial objects. In 2021, NASAâ€™s Perseverance Rover landed on Mars, showing the capability of safe autonomous landing outside Earth, furthered by other missions to Titan, the Moon, and even Asteroids (\cite{Blackmore2016}). However, rockets are much heavier than science payloads currently used in planetary landings, leading to greater sophistication within the control system, which results in a higher accuracy and the ability to land on the landing pad precisely.


Data-driven control systems can present a crucial innovation for providing an autonomous control system to overcome the challenges presented in rocket landing. Reinforcement Learning (RL) provides the possibility of creating a control system that is robust to modelling errors, adaptable to uncertain environments and can provide optimal control. This technique can offer optimal trajectories to launch vehicles while overcoming disturbances or modelling errors. For example, RL allows the system to learn through interaction, allowing it to control complex non-linear cases through a learnt policy. This benefits the complex planetary landing scenario with atmospheric uncertainty and limited data for model validation. By leveraging data-driven control techniques over non-adaptive trajectory planning methods like convex programming, a more flexible system can be developed to handle real-time changes.

\cite{Gaudet2018} applied reinforcement learning through Proximal Policy Optimisation (PPO) on a 6-degree of freedom (DoF) rocket landing problem. Merging guidance and control through a single agent; creating a policy from \textit{observations} in the form of simulated sensor inputs to enact \textit{actions} in the form of control forces and moments. Experiments showed an accurate result robust to uncertainties like noise and parameter deviations through a Monte Carlo (MC) simulation. This conceptual study shows RL's ability to control this complex scenario. 

This report aims to provide a benchmark analysis into the benefits of using a data-driven control method for the rocket landing problem, culminating into an evaluation of were reinforcement learning would be best used within the rocket landing control system.

This report is split into six main sections. First, the hypothesis, research questions, and metrics are defined in the forthcoming sections of this chapter, along with the project planning in \autoref{sec:project_planning}. Following this, a scientific article presents the main work of the report in \autoref{chp:article} and is stand-alone from the rest of the report. Afterwards, a literature study are shown in \autoref{chp:literaturestudy}, and the additional results which are not included in the article in \autoref{chp:additional_results} covering items like verification and validation. Finally, conclusions, recommendations and future work ideas are given in \autoref{chp:conclusion}.
